Layer: conv_in.weight | Size: torch.Size([320, 4, 3, 3])
Layer: conv_in.bias | Size: torch.Size([320])
Layer: time_embedding.linear_1.weight | Size: torch.Size([1280, 320])
Layer: time_embedding.linear_1.bias | Size: torch.Size([1280])
Layer: time_embedding.linear_2.weight | Size: torch.Size([1280, 1280])
Layer: time_embedding.linear_2.bias | Size: torch.Size([1280])
Layer: down_blocks.0.attentions.0.norm.weight | Size: torch.Size([320])
Layer: down_blocks.0.attentions.0.norm.bias | Size: torch.Size([320])
Layer: down_blocks.0.attentions.0.proj_in.weight | Size: torch.Size([320, 320, 1, 1])
Layer: down_blocks.0.attentions.0.proj_in.bias | Size: torch.Size([320])
Layer: down_blocks.0.attentions.0.transformer_blocks.0.norm1.weight | Size: torch.Size([320])
Layer: down_blocks.0.attentions.0.transformer_blocks.0.norm1.bias | Size: torch.Size([320])
Layer: down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.weight | Size: torch.Size([320, 320])
Layer: down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.weight | Size: torch.Size([320, 320])
Layer: down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.weight | Size: torch.Size([320, 320])
Layer: down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.weight | Size: torch.Size([320, 320])
Layer: down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.bias | Size: torch.Size([320])
Layer: down_blocks.0.attentions.0.transformer_blocks.0.norm2.weight | Size: torch.Size([320])
Layer: down_blocks.0.attentions.0.transformer_blocks.0.norm2.bias | Size: torch.Size([320])
Layer: down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.weight | Size: torch.Size([320, 320])
Layer: down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight | Size: torch.Size([320, 768])
Layer: down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight | Size: torch.Size([320, 768])
Layer: down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.weight | Size: torch.Size([320, 320])
Layer: down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.bias | Size: torch.Size([320])
Layer: down_blocks.0.attentions.0.transformer_blocks.0.norm3.weight | Size: torch.Size([320])
Layer: down_blocks.0.attentions.0.transformer_blocks.0.norm3.bias | Size: torch.Size([320])
Layer: down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.weight | Size: torch.Size([2560, 320])
Layer: down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.bias | Size: torch.Size([2560])
Layer: down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.weight | Size: torch.Size([320, 1280])
Layer: down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.bias | Size: torch.Size([320])
Layer: down_blocks.0.attentions.0.proj_out.weight | Size: torch.Size([320, 320, 1, 1])
Layer: down_blocks.0.attentions.0.proj_out.bias | Size: torch.Size([320])
Layer: down_blocks.0.resnets.0.norm1.weight | Size: torch.Size([320])
Layer: down_blocks.0.resnets.0.norm1.bias | Size: torch.Size([320])
Layer: down_blocks.0.resnets.0.conv1.weight | Size: torch.Size([320, 320, 3, 3])
Layer: down_blocks.0.resnets.0.conv1.bias | Size: torch.Size([320])
Layer: down_blocks.0.resnets.0.time_emb_proj.weight | Size: torch.Size([320, 1280])
Layer: down_blocks.0.resnets.0.time_emb_proj.bias | Size: torch.Size([320])
Layer: down_blocks.0.resnets.0.norm2.weight | Size: torch.Size([320])
Layer: down_blocks.0.resnets.0.norm2.bias | Size: torch.Size([320])
Layer: down_blocks.0.resnets.0.conv2.weight | Size: torch.Size([320, 320, 3, 3])
Layer: down_blocks.0.resnets.0.conv2.bias | Size: torch.Size([320])
Layer: down_blocks.0.downsamplers.0.conv.weight | Size: torch.Size([320, 320, 3, 3])
Layer: down_blocks.0.downsamplers.0.conv.bias | Size: torch.Size([320])
Layer: down_blocks.1.attentions.0.norm.weight | Size: torch.Size([640])
Layer: down_blocks.1.attentions.0.norm.bias | Size: torch.Size([640])
Layer: down_blocks.1.attentions.0.proj_in.weight | Size: torch.Size([640, 640, 1, 1])
Layer: down_blocks.1.attentions.0.proj_in.bias | Size: torch.Size([640])
Layer: down_blocks.1.attentions.0.transformer_blocks.0.norm1.weight | Size: torch.Size([640])
Layer: down_blocks.1.attentions.0.transformer_blocks.0.norm1.bias | Size: torch.Size([640])
Layer: down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight | Size: torch.Size([640, 640])
Layer: down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight | Size: torch.Size([640, 640])
Layer: down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight | Size: torch.Size([640, 640])
Layer: down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight | Size: torch.Size([640, 640])
Layer: down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias | Size: torch.Size([640])
Layer: down_blocks.1.attentions.0.transformer_blocks.0.norm2.weight | Size: torch.Size([640])
Layer: down_blocks.1.attentions.0.transformer_blocks.0.norm2.bias | Size: torch.Size([640])
Layer: down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight | Size: torch.Size([640, 640])
Layer: down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight | Size: torch.Size([640, 768])
Layer: down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight | Size: torch.Size([640, 768])
Layer: down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight | Size: torch.Size([640, 640])
Layer: down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias | Size: torch.Size([640])
Layer: down_blocks.1.attentions.0.transformer_blocks.0.norm3.weight | Size: torch.Size([640])
Layer: down_blocks.1.attentions.0.transformer_blocks.0.norm3.bias | Size: torch.Size([640])
Layer: down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight | Size: torch.Size([5120, 640])
Layer: down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias | Size: torch.Size([5120])
Layer: down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight | Size: torch.Size([640, 2560])
Layer: down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias | Size: torch.Size([640])
Layer: down_blocks.1.attentions.0.proj_out.weight | Size: torch.Size([640, 640, 1, 1])
Layer: down_blocks.1.attentions.0.proj_out.bias | Size: torch.Size([640])
Layer: down_blocks.1.resnets.0.norm1.weight | Size: torch.Size([320])
Layer: down_blocks.1.resnets.0.norm1.bias | Size: torch.Size([320])
Layer: down_blocks.1.resnets.0.conv1.weight | Size: torch.Size([640, 320, 3, 3])
Layer: down_blocks.1.resnets.0.conv1.bias | Size: torch.Size([640])
Layer: down_blocks.1.resnets.0.time_emb_proj.weight | Size: torch.Size([640, 1280])
Layer: down_blocks.1.resnets.0.time_emb_proj.bias | Size: torch.Size([640])
Layer: down_blocks.1.resnets.0.norm2.weight | Size: torch.Size([640])
Layer: down_blocks.1.resnets.0.norm2.bias | Size: torch.Size([640])
Layer: down_blocks.1.resnets.0.conv2.weight | Size: torch.Size([640, 640, 3, 3])
Layer: down_blocks.1.resnets.0.conv2.bias | Size: torch.Size([640])
Layer: down_blocks.1.resnets.0.conv_shortcut.weight | Size: torch.Size([640, 320, 1, 1])
Layer: down_blocks.1.resnets.0.conv_shortcut.bias | Size: torch.Size([640])
Layer: down_blocks.1.downsamplers.0.conv.weight | Size: torch.Size([640, 640, 3, 3])
Layer: down_blocks.1.downsamplers.0.conv.bias | Size: torch.Size([640])
Layer: down_blocks.2.attentions.0.norm.weight | Size: torch.Size([1280])
Layer: down_blocks.2.attentions.0.norm.bias | Size: torch.Size([1280])
Layer: down_blocks.2.attentions.0.proj_in.weight | Size: torch.Size([1280, 1280, 1, 1])
Layer: down_blocks.2.attentions.0.proj_in.bias | Size: torch.Size([1280])
Layer: down_blocks.2.attentions.0.transformer_blocks.0.norm1.weight | Size: torch.Size([1280])
Layer: down_blocks.2.attentions.0.transformer_blocks.0.norm1.bias | Size: torch.Size([1280])
Layer: down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight | Size: torch.Size([1280, 1280])
Layer: down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight | Size: torch.Size([1280, 1280])
Layer: down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight | Size: torch.Size([1280, 1280])
Layer: down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight | Size: torch.Size([1280, 1280])
Layer: down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias | Size: torch.Size([1280])
Layer: down_blocks.2.attentions.0.transformer_blocks.0.norm2.weight | Size: torch.Size([1280])
Layer: down_blocks.2.attentions.0.transformer_blocks.0.norm2.bias | Size: torch.Size([1280])
Layer: down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight | Size: torch.Size([1280, 1280])
Layer: down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight | Size: torch.Size([1280, 768])
Layer: down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight | Size: torch.Size([1280, 768])
Layer: down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight | Size: torch.Size([1280, 1280])
Layer: down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias | Size: torch.Size([1280])
Layer: down_blocks.2.attentions.0.transformer_blocks.0.norm3.weight | Size: torch.Size([1280])
Layer: down_blocks.2.attentions.0.transformer_blocks.0.norm3.bias | Size: torch.Size([1280])
Layer: down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight | Size: torch.Size([10240, 1280])
Layer: down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias | Size: torch.Size([10240])
Layer: down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight | Size: torch.Size([1280, 5120])
Layer: down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias | Size: torch.Size([1280])
Layer: down_blocks.2.attentions.0.proj_out.weight | Size: torch.Size([1280, 1280, 1, 1])
Layer: down_blocks.2.attentions.0.proj_out.bias | Size: torch.Size([1280])
Layer: down_blocks.2.resnets.0.norm1.weight | Size: torch.Size([640])
Layer: down_blocks.2.resnets.0.norm1.bias | Size: torch.Size([640])
Layer: down_blocks.2.resnets.0.conv1.weight | Size: torch.Size([1280, 640, 3, 3])
Layer: down_blocks.2.resnets.0.conv1.bias | Size: torch.Size([1280])
Layer: down_blocks.2.resnets.0.time_emb_proj.weight | Size: torch.Size([1280, 1280])
Layer: down_blocks.2.resnets.0.time_emb_proj.bias | Size: torch.Size([1280])
Layer: down_blocks.2.resnets.0.norm2.weight | Size: torch.Size([1280])
Layer: down_blocks.2.resnets.0.norm2.bias | Size: torch.Size([1280])
Layer: down_blocks.2.resnets.0.conv2.weight | Size: torch.Size([1280, 1280, 3, 3])
Layer: down_blocks.2.resnets.0.conv2.bias | Size: torch.Size([1280])
Layer: down_blocks.2.resnets.0.conv_shortcut.weight | Size: torch.Size([1280, 640, 1, 1])
Layer: down_blocks.2.resnets.0.conv_shortcut.bias | Size: torch.Size([1280])
Layer: up_blocks.0.attentions.0.norm.weight | Size: torch.Size([1280])
Layer: up_blocks.0.attentions.0.norm.bias | Size: torch.Size([1280])
Layer: up_blocks.0.attentions.0.proj_in.weight | Size: torch.Size([1280, 1280, 1, 1])
Layer: up_blocks.0.attentions.0.proj_in.bias | Size: torch.Size([1280])
Layer: up_blocks.0.attentions.0.transformer_blocks.0.norm1.weight | Size: torch.Size([1280])
Layer: up_blocks.0.attentions.0.transformer_blocks.0.norm1.bias | Size: torch.Size([1280])
Layer: up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.weight | Size: torch.Size([1280, 1280])
Layer: up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.weight | Size: torch.Size([1280, 1280])
Layer: up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.weight | Size: torch.Size([1280, 1280])
Layer: up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.weight | Size: torch.Size([1280, 1280])
Layer: up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.bias | Size: torch.Size([1280])
Layer: up_blocks.0.attentions.0.transformer_blocks.0.norm2.weight | Size: torch.Size([1280])
Layer: up_blocks.0.attentions.0.transformer_blocks.0.norm2.bias | Size: torch.Size([1280])
Layer: up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.weight | Size: torch.Size([1280, 1280])
Layer: up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight | Size: torch.Size([1280, 768])
Layer: up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight | Size: torch.Size([1280, 768])
Layer: up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.weight | Size: torch.Size([1280, 1280])
Layer: up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.bias | Size: torch.Size([1280])
Layer: up_blocks.0.attentions.0.transformer_blocks.0.norm3.weight | Size: torch.Size([1280])
Layer: up_blocks.0.attentions.0.transformer_blocks.0.norm3.bias | Size: torch.Size([1280])
Layer: up_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.weight | Size: torch.Size([10240, 1280])
Layer: up_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.bias | Size: torch.Size([10240])
Layer: up_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.weight | Size: torch.Size([1280, 5120])
Layer: up_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.bias | Size: torch.Size([1280])
Layer: up_blocks.0.attentions.0.proj_out.weight | Size: torch.Size([1280, 1280, 1, 1])
Layer: up_blocks.0.attentions.0.proj_out.bias | Size: torch.Size([1280])
Layer: up_blocks.0.attentions.1.norm.weight | Size: torch.Size([1280])
Layer: up_blocks.0.attentions.1.norm.bias | Size: torch.Size([1280])
Layer: up_blocks.0.attentions.1.proj_in.weight | Size: torch.Size([1280, 1280, 1, 1])
Layer: up_blocks.0.attentions.1.proj_in.bias | Size: torch.Size([1280])
Layer: up_blocks.0.attentions.1.transformer_blocks.0.norm1.weight | Size: torch.Size([1280])
Layer: up_blocks.0.attentions.1.transformer_blocks.0.norm1.bias | Size: torch.Size([1280])
Layer: up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.weight | Size: torch.Size([1280, 1280])
Layer: up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.weight | Size: torch.Size([1280, 1280])
Layer: up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.weight | Size: torch.Size([1280, 1280])
Layer: up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.weight | Size: torch.Size([1280, 1280])
Layer: up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.bias | Size: torch.Size([1280])
Layer: up_blocks.0.attentions.1.transformer_blocks.0.norm2.weight | Size: torch.Size([1280])
Layer: up_blocks.0.attentions.1.transformer_blocks.0.norm2.bias | Size: torch.Size([1280])
Layer: up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.weight | Size: torch.Size([1280, 1280])
Layer: up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight | Size: torch.Size([1280, 768])
Layer: up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight | Size: torch.Size([1280, 768])
Layer: up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.weight | Size: torch.Size([1280, 1280])
Layer: up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.bias | Size: torch.Size([1280])
Layer: up_blocks.0.attentions.1.transformer_blocks.0.norm3.weight | Size: torch.Size([1280])
Layer: up_blocks.0.attentions.1.transformer_blocks.0.norm3.bias | Size: torch.Size([1280])
Layer: up_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.weight | Size: torch.Size([10240, 1280])
Layer: up_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.bias | Size: torch.Size([10240])
Layer: up_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.weight | Size: torch.Size([1280, 5120])
Layer: up_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.bias | Size: torch.Size([1280])
Layer: up_blocks.0.attentions.1.proj_out.weight | Size: torch.Size([1280, 1280, 1, 1])
Layer: up_blocks.0.attentions.1.proj_out.bias | Size: torch.Size([1280])
Layer: up_blocks.0.resnets.0.norm1.weight | Size: torch.Size([2560])
Layer: up_blocks.0.resnets.0.norm1.bias | Size: torch.Size([2560])
Layer: up_blocks.0.resnets.0.conv1.weight | Size: torch.Size([1280, 2560, 3, 3])
Layer: up_blocks.0.resnets.0.conv1.bias | Size: torch.Size([1280])
Layer: up_blocks.0.resnets.0.time_emb_proj.weight | Size: torch.Size([1280, 1280])
Layer: up_blocks.0.resnets.0.time_emb_proj.bias | Size: torch.Size([1280])
Layer: up_blocks.0.resnets.0.norm2.weight | Size: torch.Size([1280])
Layer: up_blocks.0.resnets.0.norm2.bias | Size: torch.Size([1280])
Layer: up_blocks.0.resnets.0.conv2.weight | Size: torch.Size([1280, 1280, 3, 3])
Layer: up_blocks.0.resnets.0.conv2.bias | Size: torch.Size([1280])
Layer: up_blocks.0.resnets.0.conv_shortcut.weight | Size: torch.Size([1280, 2560, 1, 1])
Layer: up_blocks.0.resnets.0.conv_shortcut.bias | Size: torch.Size([1280])
Layer: up_blocks.0.resnets.1.norm1.weight | Size: torch.Size([1920])
Layer: up_blocks.0.resnets.1.norm1.bias | Size: torch.Size([1920])
Layer: up_blocks.0.resnets.1.conv1.weight | Size: torch.Size([1280, 1920, 3, 3])
Layer: up_blocks.0.resnets.1.conv1.bias | Size: torch.Size([1280])
Layer: up_blocks.0.resnets.1.time_emb_proj.weight | Size: torch.Size([1280, 1280])
Layer: up_blocks.0.resnets.1.time_emb_proj.bias | Size: torch.Size([1280])
Layer: up_blocks.0.resnets.1.norm2.weight | Size: torch.Size([1280])
Layer: up_blocks.0.resnets.1.norm2.bias | Size: torch.Size([1280])
Layer: up_blocks.0.resnets.1.conv2.weight | Size: torch.Size([1280, 1280, 3, 3])
Layer: up_blocks.0.resnets.1.conv2.bias | Size: torch.Size([1280])
Layer: up_blocks.0.resnets.1.conv_shortcut.weight | Size: torch.Size([1280, 1920, 1, 1])
Layer: up_blocks.0.resnets.1.conv_shortcut.bias | Size: torch.Size([1280])
Layer: up_blocks.0.upsamplers.0.conv.weight | Size: torch.Size([1280, 1280, 3, 3])
Layer: up_blocks.0.upsamplers.0.conv.bias | Size: torch.Size([1280])
Layer: up_blocks.1.attentions.0.norm.weight | Size: torch.Size([640])
Layer: up_blocks.1.attentions.0.norm.bias | Size: torch.Size([640])
Layer: up_blocks.1.attentions.0.proj_in.weight | Size: torch.Size([640, 640, 1, 1])
Layer: up_blocks.1.attentions.0.proj_in.bias | Size: torch.Size([640])
Layer: up_blocks.1.attentions.0.transformer_blocks.0.norm1.weight | Size: torch.Size([640])
Layer: up_blocks.1.attentions.0.transformer_blocks.0.norm1.bias | Size: torch.Size([640])
Layer: up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight | Size: torch.Size([640, 640])
Layer: up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight | Size: torch.Size([640, 640])
Layer: up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight | Size: torch.Size([640, 640])
Layer: up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight | Size: torch.Size([640, 640])
Layer: up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias | Size: torch.Size([640])
Layer: up_blocks.1.attentions.0.transformer_blocks.0.norm2.weight | Size: torch.Size([640])
Layer: up_blocks.1.attentions.0.transformer_blocks.0.norm2.bias | Size: torch.Size([640])
Layer: up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight | Size: torch.Size([640, 640])
Layer: up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight | Size: torch.Size([640, 768])
Layer: up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight | Size: torch.Size([640, 768])
Layer: up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight | Size: torch.Size([640, 640])
Layer: up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias | Size: torch.Size([640])
Layer: up_blocks.1.attentions.0.transformer_blocks.0.norm3.weight | Size: torch.Size([640])
Layer: up_blocks.1.attentions.0.transformer_blocks.0.norm3.bias | Size: torch.Size([640])
Layer: up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight | Size: torch.Size([5120, 640])
Layer: up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias | Size: torch.Size([5120])
Layer: up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight | Size: torch.Size([640, 2560])
Layer: up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias | Size: torch.Size([640])
Layer: up_blocks.1.attentions.0.proj_out.weight | Size: torch.Size([640, 640, 1, 1])
Layer: up_blocks.1.attentions.0.proj_out.bias | Size: torch.Size([640])
Layer: up_blocks.1.attentions.1.norm.weight | Size: torch.Size([640])
Layer: up_blocks.1.attentions.1.norm.bias | Size: torch.Size([640])
Layer: up_blocks.1.attentions.1.proj_in.weight | Size: torch.Size([640, 640, 1, 1])
Layer: up_blocks.1.attentions.1.proj_in.bias | Size: torch.Size([640])
Layer: up_blocks.1.attentions.1.transformer_blocks.0.norm1.weight | Size: torch.Size([640])
Layer: up_blocks.1.attentions.1.transformer_blocks.0.norm1.bias | Size: torch.Size([640])
Layer: up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight | Size: torch.Size([640, 640])
Layer: up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight | Size: torch.Size([640, 640])
Layer: up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight | Size: torch.Size([640, 640])
Layer: up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight | Size: torch.Size([640, 640])
Layer: up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias | Size: torch.Size([640])
Layer: up_blocks.1.attentions.1.transformer_blocks.0.norm2.weight | Size: torch.Size([640])
Layer: up_blocks.1.attentions.1.transformer_blocks.0.norm2.bias | Size: torch.Size([640])
Layer: up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight | Size: torch.Size([640, 640])
Layer: up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight | Size: torch.Size([640, 768])
Layer: up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight | Size: torch.Size([640, 768])
Layer: up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight | Size: torch.Size([640, 640])
Layer: up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias | Size: torch.Size([640])
Layer: up_blocks.1.attentions.1.transformer_blocks.0.norm3.weight | Size: torch.Size([640])
Layer: up_blocks.1.attentions.1.transformer_blocks.0.norm3.bias | Size: torch.Size([640])
Layer: up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight | Size: torch.Size([5120, 640])
Layer: up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias | Size: torch.Size([5120])
Layer: up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight | Size: torch.Size([640, 2560])
Layer: up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias | Size: torch.Size([640])
Layer: up_blocks.1.attentions.1.proj_out.weight | Size: torch.Size([640, 640, 1, 1])
Layer: up_blocks.1.attentions.1.proj_out.bias | Size: torch.Size([640])
Layer: up_blocks.1.resnets.0.norm1.weight | Size: torch.Size([1920])
Layer: up_blocks.1.resnets.0.norm1.bias | Size: torch.Size([1920])
Layer: up_blocks.1.resnets.0.conv1.weight | Size: torch.Size([640, 1920, 3, 3])
Layer: up_blocks.1.resnets.0.conv1.bias | Size: torch.Size([640])
Layer: up_blocks.1.resnets.0.time_emb_proj.weight | Size: torch.Size([640, 1280])
Layer: up_blocks.1.resnets.0.time_emb_proj.bias | Size: torch.Size([640])
Layer: up_blocks.1.resnets.0.norm2.weight | Size: torch.Size([640])
Layer: up_blocks.1.resnets.0.norm2.bias | Size: torch.Size([640])
Layer: up_blocks.1.resnets.0.conv2.weight | Size: torch.Size([640, 640, 3, 3])
Layer: up_blocks.1.resnets.0.conv2.bias | Size: torch.Size([640])
Layer: up_blocks.1.resnets.0.conv_shortcut.weight | Size: torch.Size([640, 1920, 1, 1])
Layer: up_blocks.1.resnets.0.conv_shortcut.bias | Size: torch.Size([640])
Layer: up_blocks.1.resnets.1.norm1.weight | Size: torch.Size([960])
Layer: up_blocks.1.resnets.1.norm1.bias | Size: torch.Size([960])
Layer: up_blocks.1.resnets.1.conv1.weight | Size: torch.Size([640, 960, 3, 3])
Layer: up_blocks.1.resnets.1.conv1.bias | Size: torch.Size([640])
Layer: up_blocks.1.resnets.1.time_emb_proj.weight | Size: torch.Size([640, 1280])
Layer: up_blocks.1.resnets.1.time_emb_proj.bias | Size: torch.Size([640])
Layer: up_blocks.1.resnets.1.norm2.weight | Size: torch.Size([640])
Layer: up_blocks.1.resnets.1.norm2.bias | Size: torch.Size([640])
Layer: up_blocks.1.resnets.1.conv2.weight | Size: torch.Size([640, 640, 3, 3])
Layer: up_blocks.1.resnets.1.conv2.bias | Size: torch.Size([640])
Layer: up_blocks.1.resnets.1.conv_shortcut.weight | Size: torch.Size([640, 960, 1, 1])
Layer: up_blocks.1.resnets.1.conv_shortcut.bias | Size: torch.Size([640])
Layer: up_blocks.1.upsamplers.0.conv.weight | Size: torch.Size([640, 640, 3, 3])
Layer: up_blocks.1.upsamplers.0.conv.bias | Size: torch.Size([640])
Layer: up_blocks.2.attentions.0.norm.weight | Size: torch.Size([320])
Layer: up_blocks.2.attentions.0.norm.bias | Size: torch.Size([320])
Layer: up_blocks.2.attentions.0.proj_in.weight | Size: torch.Size([320, 320, 1, 1])
Layer: up_blocks.2.attentions.0.proj_in.bias | Size: torch.Size([320])
Layer: up_blocks.2.attentions.0.transformer_blocks.0.norm1.weight | Size: torch.Size([320])
Layer: up_blocks.2.attentions.0.transformer_blocks.0.norm1.bias | Size: torch.Size([320])
Layer: up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight | Size: torch.Size([320, 320])
Layer: up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight | Size: torch.Size([320, 320])
Layer: up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight | Size: torch.Size([320, 320])
Layer: up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight | Size: torch.Size([320, 320])
Layer: up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias | Size: torch.Size([320])
Layer: up_blocks.2.attentions.0.transformer_blocks.0.norm2.weight | Size: torch.Size([320])
Layer: up_blocks.2.attentions.0.transformer_blocks.0.norm2.bias | Size: torch.Size([320])
Layer: up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight | Size: torch.Size([320, 320])
Layer: up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight | Size: torch.Size([320, 768])
Layer: up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight | Size: torch.Size([320, 768])
Layer: up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight | Size: torch.Size([320, 320])
Layer: up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias | Size: torch.Size([320])
Layer: up_blocks.2.attentions.0.transformer_blocks.0.norm3.weight | Size: torch.Size([320])
Layer: up_blocks.2.attentions.0.transformer_blocks.0.norm3.bias | Size: torch.Size([320])
Layer: up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight | Size: torch.Size([2560, 320])
Layer: up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias | Size: torch.Size([2560])
Layer: up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight | Size: torch.Size([320, 1280])
Layer: up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias | Size: torch.Size([320])
Layer: up_blocks.2.attentions.0.proj_out.weight | Size: torch.Size([320, 320, 1, 1])
Layer: up_blocks.2.attentions.0.proj_out.bias | Size: torch.Size([320])
Layer: up_blocks.2.attentions.1.norm.weight | Size: torch.Size([320])
Layer: up_blocks.2.attentions.1.norm.bias | Size: torch.Size([320])
Layer: up_blocks.2.attentions.1.proj_in.weight | Size: torch.Size([320, 320, 1, 1])
Layer: up_blocks.2.attentions.1.proj_in.bias | Size: torch.Size([320])
Layer: up_blocks.2.attentions.1.transformer_blocks.0.norm1.weight | Size: torch.Size([320])
Layer: up_blocks.2.attentions.1.transformer_blocks.0.norm1.bias | Size: torch.Size([320])
Layer: up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight | Size: torch.Size([320, 320])
Layer: up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight | Size: torch.Size([320, 320])
Layer: up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight | Size: torch.Size([320, 320])
Layer: up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight | Size: torch.Size([320, 320])
Layer: up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias | Size: torch.Size([320])
Layer: up_blocks.2.attentions.1.transformer_blocks.0.norm2.weight | Size: torch.Size([320])
Layer: up_blocks.2.attentions.1.transformer_blocks.0.norm2.bias | Size: torch.Size([320])
Layer: up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight | Size: torch.Size([320, 320])
Layer: up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight | Size: torch.Size([320, 768])
Layer: up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight | Size: torch.Size([320, 768])
Layer: up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight | Size: torch.Size([320, 320])
Layer: up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias | Size: torch.Size([320])
Layer: up_blocks.2.attentions.1.transformer_blocks.0.norm3.weight | Size: torch.Size([320])
Layer: up_blocks.2.attentions.1.transformer_blocks.0.norm3.bias | Size: torch.Size([320])
Layer: up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight | Size: torch.Size([2560, 320])
Layer: up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias | Size: torch.Size([2560])
Layer: up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight | Size: torch.Size([320, 1280])
Layer: up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias | Size: torch.Size([320])
Layer: up_blocks.2.attentions.1.proj_out.weight | Size: torch.Size([320, 320, 1, 1])
Layer: up_blocks.2.attentions.1.proj_out.bias | Size: torch.Size([320])
Layer: up_blocks.2.resnets.0.norm1.weight | Size: torch.Size([960])
Layer: up_blocks.2.resnets.0.norm1.bias | Size: torch.Size([960])
Layer: up_blocks.2.resnets.0.conv1.weight | Size: torch.Size([320, 960, 3, 3])
Layer: up_blocks.2.resnets.0.conv1.bias | Size: torch.Size([320])
Layer: up_blocks.2.resnets.0.time_emb_proj.weight | Size: torch.Size([320, 1280])
Layer: up_blocks.2.resnets.0.time_emb_proj.bias | Size: torch.Size([320])
Layer: up_blocks.2.resnets.0.norm2.weight | Size: torch.Size([320])
Layer: up_blocks.2.resnets.0.norm2.bias | Size: torch.Size([320])
Layer: up_blocks.2.resnets.0.conv2.weight | Size: torch.Size([320, 320, 3, 3])
Layer: up_blocks.2.resnets.0.conv2.bias | Size: torch.Size([320])
Layer: up_blocks.2.resnets.0.conv_shortcut.weight | Size: torch.Size([320, 960, 1, 1])
Layer: up_blocks.2.resnets.0.conv_shortcut.bias | Size: torch.Size([320])
Layer: up_blocks.2.resnets.1.norm1.weight | Size: torch.Size([640])
Layer: up_blocks.2.resnets.1.norm1.bias | Size: torch.Size([640])
Layer: up_blocks.2.resnets.1.conv1.weight | Size: torch.Size([320, 640, 3, 3])
Layer: up_blocks.2.resnets.1.conv1.bias | Size: torch.Size([320])
Layer: up_blocks.2.resnets.1.time_emb_proj.weight | Size: torch.Size([320, 1280])
Layer: up_blocks.2.resnets.1.time_emb_proj.bias | Size: torch.Size([320])
Layer: up_blocks.2.resnets.1.norm2.weight | Size: torch.Size([320])
Layer: up_blocks.2.resnets.1.norm2.bias | Size: torch.Size([320])
Layer: up_blocks.2.resnets.1.conv2.weight | Size: torch.Size([320, 320, 3, 3])
Layer: up_blocks.2.resnets.1.conv2.bias | Size: torch.Size([320])
Layer: up_blocks.2.resnets.1.conv_shortcut.weight | Size: torch.Size([320, 640, 1, 1])
Layer: up_blocks.2.resnets.1.conv_shortcut.bias | Size: torch.Size([320])
Layer: conv_norm_out.weight | Size: torch.Size([320])
Layer: conv_norm_out.bias | Size: torch.Size([320])
Layer: conv_out.weight | Size: torch.Size([4, 320, 3, 3])
Layer: conv_out.bias | Size: torch.Size([4])